train_name: dual_xception_higherlr

# RETRAIN
retrain: 1
premodel_path: torch_models/dual_xception_higherlr_v01/best_avg_acc_model.pth
premetric_path: torch_models/dual_xception_higherlr_v01/best_avg_acc_metric_fc.pth

main_data_dir: ../all_train_300
use_train_dir: full_train
main_model_dir: torch_models
workers: 8
size: 400
batch_size: 4
batch_multiplier: 15
reweight_sample: 1
reweight_sample_factor: 0.025

epochs: 200

loss_type: cross_entropy
model_type: dual_xception
model_kwargs:
  pretrained: True

metric_type: softmax
num_ftr: 1000
num_classes: 5

optimizer_type: AdamW
opt_kwargs:
  lr: 0.0005
  weight_decay: 0.00025

# optimizer_type: SGD
# opt_kwargs:
#   lr: 0.1
#   momentum: 0.95
#   # weight_decay: 0.0001
#   weight_decay: 0.0025


# scheduler_type: CosineAnnealingWarmRestarts
# scheduler_kwargs:
#   T_0: 10
#   T_mult: 2
#   eta_min: 0.00001
#   last_epoch: -1

scheduler_type: CosineAnnealingLR
scheduler_kwargs:
  T_max: 200
  eta_min: 0.00005

# https://www.fast.ai/2018/07/02/adam-weight-decay/#implementing-adamw
# adamw with onecycle
