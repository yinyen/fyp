# Init active learning
train_name: dual_active_learning
style: random

# root_dir: "AL"
root_dir: "./Dual_AL"
main_dir: "dual_al_random"
main_data_dir: ../all_train_300
train_dir: full_train

max_steps: 50

# active learning parameters
m: 200 # start
n: 100  # step sampling
outlier: 0.01
# to vary:

# preprocessing
workers: 8
size: 400
reweight_sample: 1 #-1 #resample limited label images to 1k images with oversampling
reweight_sample_factor: 2 # change to factor

# model
model_type: dual_xception
epochs: 200 # max epoch to run
patience: 10 # implement patience to stop training if val doesnt improve
batch_size: 3
batch_multiplier: 20

# loss
loss_type: cross_entropy

# metric 
metric_type: softmax
num_ftr: 1000
num_classes: 5

# optimizer
optimizer_type: AdamW
opt_kwargs:
  lr: 0.0005
  weight_decay: 0.00025


# optimizer_type: SGD
# opt_kwargs:
#   lr: 0.01
#   momentum: 0.95
#   # weight_decay: 0.0001
#   weight_decay: 0.0025

model_kwargs:
  pretrained: True
scheduler_type: CosineAnnealingLR
scheduler_kwargs:
  T_max: 200
  eta_min: 0.00005

# https://www.fast.ai/2018/07/02/adam-weight-decay/#implementing-adamw
# adamw with onecycle
