train_name: new_small_dual_xception_400_v7mp

# RETRAIN
retrain: 0
premodel_path: torch_models/new_small_dual_xception_400_v3a_v13/best_qk_model.pth

main_data_dir: ../all_train_300
use_train_dir: full_train
main_model_dir: torch_models
workers: 8
size: 400
batch_size: 6
batch_multiplier: 10
reweight_sample: -1
reweight_sample_factor: 2

epochs: 200

# loss_type: cross_entropy
# loss_type: my_cross_entropy
loss_type: smoothl1loss
model_type: new_small_dual_xception
model_kwargs:
  num_ftrs: 1000
  num_classes: 1
  use_init: kaiming
  limit_two: 1
qk_patience: 20
  # mean: 3.3
  # std: 3.0003
  # mean: 0.003
  # std: 0.0003

optimizer_type: AdamW
opt_kwargs:
  lr: 0.0002
  weight_decay: 0.000025

# optimizer_type: SGD
# opt_kwargs:
#   lr: 0.0001
#   momentum: 0.95
#   weight_decay: 0.000025


# scheduler_type: CosineAnnealingWarmRestarts
# scheduler_kwargs:
#   T_0: 10
#   T_mult: 2
#   eta_min: 0.00001
#   last_epoch: -1

scheduler_type: CosineAnnealingLR
scheduler_kwargs:
  T_max: 200
  # eta_min: 0.000005
  eta_min: 0.0001

# https://www.fast.ai/2018/07/02/adam-weight-decay/#implementing-adamw
# adamw with onecycle
